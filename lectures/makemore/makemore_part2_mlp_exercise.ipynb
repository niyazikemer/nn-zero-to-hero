{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`MAKEMORE PART2 TURKISH`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aba', 'abaca', 'abacan', 'abaç', 'abay', 'abayhan', 'abaza', 'abbas']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('turkce_isim.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    names = [row[0] for row in reader]\n",
    "    names.pop(0)\n",
    "\n",
    "print(names[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aba', 'abaca', 'abacan', 'abaç', 'abay']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = names\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'ç': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'ğ': 9, 'h': 10, 'ı': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'ö': 19, 'p': 20, 'r': 21, 's': 22, 'ş': 23, 't': 24, 'u': 25, 'ü': 26, 'v': 27, 'w': 28, 'x': 29, 'y': 30, 'z': 31}\n",
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'ç', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'ğ', 10: 'h', 11: 'ı', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'ö', 20: 'p', 21: 'r', 22: 's', 23: 'ş', 24: 't', 25: 'u', 26: 'ü', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n"
     ]
    }
   ],
   "source": [
    "turkish_sort = ['a', 'b', 'c', 'ç', 'd', 'e', 'f', 'g', 'ğ', 'h', 'ı', 'i', 'j', 'k', 'l', 'm', 'n', 'o','ö', 'p', 'r', 's', 'ş', 't', 'u','ü', 'v', 'w', 'x', 'y', 'z']\n",
    "turkish_sort.insert(0,'.')\n",
    "turkish_sort,len(turkish_sort)\n",
    "stoi = {ch:i for i,ch in enumerate(turkish_sort)}\n",
    "itos = {i: st for st, i in stoi.items()}\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`Dataset`:**\n",
    "**xx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abdal\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> a\n",
      "bda ===> l\n",
      "dal ===> .\n",
      "abdi\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> i\n",
      "bdi ===> .\n",
      "abdullah\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> u\n",
      "bdu ===> l\n",
      "dul ===> l\n",
      "ull ===> a\n",
      "lla ===> h\n",
      "lah ===> .\n",
      "abdurrahman\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> u\n",
      "bdu ===> r\n",
      "dur ===> r\n",
      "urr ===> a\n",
      "rra ===> h\n",
      "rah ===> m\n",
      "ahm ===> a\n",
      "hma ===> n\n",
      "man ===> .\n",
      "abdülalim\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> ü\n",
      "bdü ===> l\n",
      "dül ===> a\n",
      "üla ===> l\n",
      "lal ===> i\n",
      "ali ===> m\n",
      "lim ===> .\n",
      "abdülazim\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> ü\n",
      "bdü ===> l\n",
      "dül ===> a\n",
      "üla ===> z\n",
      "laz ===> i\n",
      "azi ===> m\n",
      "zim ===> .\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "context = [0]*block_size\n",
    "X, Y = [], []\n",
    "for w in words[8:14]:\n",
    "    context = [0]*block_size\n",
    "    print(w)\n",
    "    for le in w + '.':        \n",
    "        ix= stoi[le]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '===>', itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 3]), torch.int64, torch.Size([52]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Important`:**\n",
    "**What I find important about the second part is embedding. In the paper BENGIO at al. used 30 dimensional space for each word. Here Andrej uses 2 dimensional space for a letter. Which was not available in the previous example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Embeddings`:**\n",
    "**Just two floating number for and integer(index of a character.)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn(len(turkish_sort), 2)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`One hot encoding`:**\n",
    "**understanding the effect of one hot vector turing matrix multiplication.Simply only `one` in the vector effects the matrix multiplication.**\n",
    "```python\n",
    "F.one_hot(torch.tensor(5), num_classes=len(turkish_sort)).float() @ C\n",
    "```\n",
    "**just plucks out the 5th index from the `C` lockup tensor**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0016, 1.1534])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=len(turkish_sort)).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0016,  1.1534],\n",
       "        [ 0.4804, -0.7848]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for getting more than one index from C then it is possible\n",
    "# to use list of the indexes to the one_hot function\n",
    "# the result will be embedded vectors of the indexes\n",
    "one_hot = F.one_hot(torch.tensor([5, 6]), num_classes=len(turkish_sort)).float() @ C\n",
    "one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the shape represents at the 0th index the number of embeddings\n",
    "# and at the 1st index the number of the dimensions of the embedding\n",
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 3, 2]),\n",
       " tensor([[[ 0.4213, -1.5737],\n",
       "          [ 0.4213, -1.5737],\n",
       "          [ 0.4213, -1.5737]],\n",
       " \n",
       "         [[ 0.4213, -1.5737],\n",
       "          [ 0.4213, -1.5737],\n",
       "          [-0.2250, -0.0521]],\n",
       " \n",
       "         [[ 0.4213, -1.5737],\n",
       "          [-0.2250, -0.0521],\n",
       "          [-1.0109, -0.7784]]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As you put your training examples to the lookup table you will get the embeddings of the examples \n",
    "C[X].shape, C[X][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# six is for the 3 letter and 2 embeddings\n",
    "# 100 is arbitrary number of the hidden layer\n",
    "# b1 is the bias for the hidden layer\n",
    "W1 = torch.randn(6,100)\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`This section look like a mini pytorch course like in the previous lecture mentioning torch.sum() `:**\n",
    "#### **Understand the `cat`(concatenate) `unbind`, `view` operations:**\n",
    "**x**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 3, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6225,  0.8961]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Understand the `cat`(concatenate):**\n",
    "**Pretty straighhforward dimension is the key, '0' means as rows and '1' means as columns if there were two dimensions.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6225,  0.8961],\n",
       "        [-0.6225,  0.8961],\n",
       "        [-0.6225,  0.8961]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x,x,x),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Understand the `unbind`:**\n",
    "**x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6225,  0.8961],\n",
       "        [-0.6225,  0.8961],\n",
       "        [-0.6225,  0.8961]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = torch.cat((x,x,x),0)\n",
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.6225,  0.8961]),\n",
       " tensor([-0.6225,  0.8961]),\n",
       " tensor([-0.6225,  0.8961]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(x3,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Understand the `view`:**\n",
    "**x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2810,  0.2375],\n",
       "         [-0.0231, -0.7654],\n",
       "         [-0.3987,  0.9318]],\n",
       "\n",
       "        [[-2.1862,  0.1347],\n",
       "         [ 0.1889, -0.2003],\n",
       "         [-1.7675, -0.5961]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first three characters of a names looks like this for two names:\n",
    "\n",
    "first_three_of_first  = torch.randn(1,3,2)\n",
    "\n",
    "first_three_of_second  = torch.randn(1,3,2)\n",
    "\n",
    "together = torch.cat((first_three_of_first,first_three_of_second),0)\n",
    "together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2810,  0.2375, -0.0231, -0.7654, -0.3987,  0.9318],\n",
       "        [-2.1862,  0.1347,  0.1889, -0.2003, -1.7675, -0.5961]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to conver them to a row.But without needing the first dimension which is 2 that represent different names.\n",
    "together.view(-1,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Important: how bias vector added`:**\n",
    "**This is how all of the activations added not the same number but the same vector. I guess each number on the vector added to the consecutive neuron activation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`grad=None`:**\n",
    "**During the forward pass, as far as I understand we do not record gradient because before backward pass we just make it None and as far as I understand we do not use it. I guess gradien formed hust after loss is calculated. Hope to see it during the backprop lecture** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
