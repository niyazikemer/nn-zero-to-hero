{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`MAKEMORE PART2 TURKISH`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aba', 'abaca', 'abacan', 'abaç', 'abay', 'abayhan', 'abaza', 'abbas']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('turkce_isim.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    names = [row[0] for row in reader]\n",
    "    names.pop(0)\n",
    "\n",
    "print(names[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aba', 'abaca', 'abacan', 'abaç', 'abay']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = names\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'ç': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'ğ': 9, 'h': 10, 'ı': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'ö': 19, 'p': 20, 'r': 21, 's': 22, 'ş': 23, 't': 24, 'u': 25, 'ü': 26, 'v': 27, 'w': 28, 'x': 29, 'y': 30, 'z': 31}\n",
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'ç', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'ğ', 10: 'h', 11: 'ı', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'ö', 20: 'p', 21: 'r', 22: 's', 23: 'ş', 24: 't', 25: 'u', 26: 'ü', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n"
     ]
    }
   ],
   "source": [
    "turkish_sort = ['a', 'b', 'c', 'ç', 'd', 'e', 'f', 'g', 'ğ', 'h', 'ı', 'i', 'j', 'k', 'l', 'm', 'n', 'o','ö', 'p', 'r', 's', 'ş', 't', 'u','ü', 'v', 'w', 'x', 'y', 'z']\n",
    "turkish_sort.insert(0,'.')\n",
    "turkish_sort,len(turkish_sort)\n",
    "stoi = {ch:i for i,ch in enumerate(turkish_sort)}\n",
    "itos = {i: st for st, i in stoi.items()}\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`Dataset`:**\n",
    "**xx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abdal\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> a\n",
      "bda ===> l\n",
      "dal ===> .\n",
      "abdi\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> i\n",
      "bdi ===> .\n",
      "abdullah\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> u\n",
      "bdu ===> l\n",
      "dul ===> l\n",
      "ull ===> a\n",
      "lla ===> h\n",
      "lah ===> .\n",
      "abdurrahman\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> u\n",
      "bdu ===> r\n",
      "dur ===> r\n",
      "urr ===> a\n",
      "rra ===> h\n",
      "rah ===> m\n",
      "ahm ===> a\n",
      "hma ===> n\n",
      "man ===> .\n",
      "abdülalim\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> ü\n",
      "bdü ===> l\n",
      "dül ===> a\n",
      "üla ===> l\n",
      "lal ===> i\n",
      "ali ===> m\n",
      "lim ===> .\n",
      "abdülazim\n",
      "... ===> a\n",
      "..a ===> b\n",
      ".ab ===> d\n",
      "abd ===> ü\n",
      "bdü ===> l\n",
      "dül ===> a\n",
      "üla ===> z\n",
      "laz ===> i\n",
      "azi ===> m\n",
      "zim ===> .\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "context = [0]*block_size\n",
    "X, Y = [], []\n",
    "for w in words[8:14]:\n",
    "    context = [0]*block_size\n",
    "    print(w)\n",
    "    for le in w + '.':        \n",
    "        ix= stoi[le]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '===>', itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 3]), torch.int64, torch.Size([52]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Important`:**\n",
    "**What I find important about the second part is embedding. In the paper BENGIO at al. used 30 dimensional space for each word. Here Andrej uses 2 dimensional space for a letter. Which was not available in the previous example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Embeddings`:**\n",
    "**Just two floating number for and integer(index of a character.)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn(len(turkish_sort), 2)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`One hot encoding`:**\n",
    "**understanding the effect of one hot vector turing matrix multiplication.Simply only `one` in the vector effects the matrix multiplication.**\n",
    "```python\n",
    "F.one_hot(torch.tensor(5), num_classes=len(turkish_sort)).float() @ C\n",
    "```\n",
    "**just plucks out the 5th index from the `C` lockup tensor**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5029, -0.4986])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=len(turkish_sort)).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5029, -0.4986],\n",
       "        [ 0.6134, -0.8831]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for getting more than one index from C then it is possible\n",
    "# to use list of the indexes to the one_hot function\n",
    "# the result will be embedded vectors of the indexes\n",
    "one_hot = F.one_hot(torch.tensor([5, 6]), num_classes=len(turkish_sort)).float() @ C\n",
    "one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the shape represents at the 0th index the number of embeddings\n",
    "# and at the 1st index the number of the dimensions of the embedding\n",
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 3, 2]),\n",
       " tensor([[[ 1.4203, -0.4910],\n",
       "          [ 1.4203, -0.4910],\n",
       "          [ 1.4203, -0.4910]],\n",
       " \n",
       "         [[ 1.4203, -0.4910],\n",
       "          [ 1.4203, -0.4910],\n",
       "          [-0.5457,  1.6587]],\n",
       " \n",
       "         [[ 1.4203, -0.4910],\n",
       "          [-0.5457,  1.6587],\n",
       "          [-0.1320, -0.0494]]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As you put your training examples to the lookup table you will get the embeddings of the examples \n",
    "C[X].shape, C[X][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# six is for the 3 letter and 2 embeddings\n",
    "# 100 is arbitrary number of the hidden layer\n",
    "# b1 is the bias for the hidden layer\n",
    "W1 = torch.randn(6,100)\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`This section look like a mini pytorch course like in the previous lecture mentioning torch.sum() `:**\n",
    "#### **Understand the `cat`(concatenate) `unbind`, `view` operations:**\n",
    "**x**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 3, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4996, -0.1410]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Understand the `cat`(concatenate):**\n",
    "**Pretty straighhforward dimension is the key, '0' means as rows and '1' means as columns if there were two dimensions.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4996, -0.1410],\n",
       "        [ 1.4996, -0.1410],\n",
       "        [ 1.4996, -0.1410]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x,x,x),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Understand the `unbind`:**\n",
    "**x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4996, -0.1410],\n",
       "        [ 1.4996, -0.1410],\n",
       "        [ 1.4996, -0.1410]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = torch.cat((x,x,x),0)\n",
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.4996, -0.1410]),\n",
       " tensor([ 1.4996, -0.1410]),\n",
       " tensor([ 1.4996, -0.1410]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(x3,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Understand the `view`:**\n",
    "**x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1736,  0.3010],\n",
       "         [ 0.5734,  0.4294],\n",
       "         [ 1.4347, -0.9318]],\n",
       "\n",
       "        [[-0.2327, -0.0586],\n",
       "         [-0.1716, -0.2636],\n",
       "         [ 2.1780,  1.3951]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first three characters of a names looks like this for two names:\n",
    "\n",
    "first_three_of_first  = torch.randn(1,3,2)\n",
    "\n",
    "first_three_of_second  = torch.randn(1,3,2)\n",
    "\n",
    "together = torch.cat((first_three_of_first,first_three_of_second),0)\n",
    "together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1736,  0.3010,  0.5734,  0.4294,  1.4347, -0.9318],\n",
       "        [-0.2327, -0.0586, -0.1716, -0.2636,  2.1780,  1.3951]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to conver them to a row.But without needing the first dimension which is 2 that represent different names.\n",
    "together.view(-1,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Important: how bias vector added`:**\n",
    "**This is how all of the activations added not the same number but the same vector. I guess each number on the vector added to the consecutive neuron activation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0723, -0.5861, -1.0000,  ..., -0.9805,  0.7936,  0.6151],\n",
       "         [-0.5851, -0.3529, -0.9977,  ..., -0.9802, -0.7314,  0.9998],\n",
       "         [-0.9896,  0.7565,  0.9536,  ..., -0.2332,  0.9700,  0.9146],\n",
       "         ...,\n",
       "         [-0.6427,  0.6573,  0.9874,  ..., -0.5188,  0.1763,  0.9906],\n",
       "         [ 0.9975,  0.3514,  0.9932,  ...,  0.7096, -0.9998,  0.9555],\n",
       "         [ 0.2339, -0.0394,  0.8533,  ...,  0.0878, -0.9922, -0.8178]]),\n",
       " torch.Size([52, 100]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
    "h, h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 =torch.randn(100, len(turkish_sort))\n",
    "b2 = torch.randn(len(turkish_sort))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-19.5141,  10.8293,  -1.6469,  ...,  -8.2664,  -3.5521,  -1.6764],\n",
       "         [ -4.0797,  16.5948,   0.4707,  ...,  -4.4218,   7.5601,  -6.4462],\n",
       "         [  5.0019,  -1.2570,  -6.4912,  ...,   1.0800,   8.2391,  16.5117],\n",
       "         ...,\n",
       "         [  6.6508,   0.2585,  -1.6568,  ...,   3.8307,  12.4436,  13.3933],\n",
       "         [  5.8239,   2.0466,   3.6497,  ...,  20.4477,   9.0831,  14.5781],\n",
       "         [  2.8911,   2.3771,  -6.1821,  ...,  12.4109,   3.7470,  12.2366]]),\n",
       " torch.Size([52, 32]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2\n",
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.8247e-19, 8.7752e-06, 3.3488e-11,  ..., 4.4675e-14, 4.9831e-12,\n",
       "          3.2516e-11],\n",
       "         [4.4979e-13, 4.2837e-04, 4.2582e-11,  ..., 3.1947e-13, 5.1066e-08,\n",
       "          4.2195e-14],\n",
       "         [9.7736e-06, 1.8701e-08, 9.9693e-11,  ..., 1.9355e-07, 2.4884e-04,\n",
       "          9.7427e-01],\n",
       "         ...,\n",
       "         [2.4071e-04, 4.0302e-07, 5.9368e-08,  ..., 1.4346e-05, 7.8933e-02,\n",
       "          2.0404e-01],\n",
       "         [4.4420e-07, 1.0165e-08, 5.0502e-08,  ..., 9.9682e-01, 1.1562e-05,\n",
       "          2.8148e-03],\n",
       "         [1.7078e-05, 1.0215e-05, 1.9588e-09,  ..., 2.3272e-01, 4.0193e-05,\n",
       "          1.9549e-01]]),\n",
       " torch.Size([52, 32]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(-1, keepdim=True)\n",
    "probs, probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`grad=None`:**\n",
    "**During the forward pass, as far as I understand we do not record gradient because before backward pass we just make it None and as far as I understand we do not use it. I guess gradien formed hust after loss is calculated. Hope to see it during the backprop lecture** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(len(turkish_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([8.7752e-06, 4.2582e-11, 5.4013e-09, 3.6486e-01, 3.2696e-07, 4.6897e-08,\n",
       "         8.7752e-06, 4.2582e-11, 5.4013e-09, 1.3505e-04, 6.0203e-06, 8.7752e-06,\n",
       "         4.2582e-11, 5.4013e-09, 2.2334e-03, 9.6854e-06, 1.0000e+00, 8.3443e-08,\n",
       "         4.2777e-18, 1.8286e-04, 8.7752e-06, 4.2582e-11, 5.4013e-09, 2.2334e-03,\n",
       "         5.3923e-09, 7.9711e-12, 1.4610e-12, 8.9486e-12, 3.3427e-02, 5.4765e-01,\n",
       "         2.3400e-13, 1.8972e-05, 8.7752e-06, 4.2582e-11, 5.4013e-09, 4.0599e-06,\n",
       "         3.6201e-08, 7.5608e-15, 2.4239e-05, 4.9091e-07, 1.5448e-04, 6.6624e-05,\n",
       "         8.7752e-06, 4.2582e-11, 5.4013e-09, 4.0599e-06, 3.6201e-08, 7.5608e-15,\n",
       "         1.7931e-10, 1.8331e-06, 1.0411e-06, 1.7078e-05]),\n",
       " torch.Size([52]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(len(Y)), Y], probs[torch.arange(len(Y)), Y].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**these are the probabilities of the correct letters but it looks mostly very low because the model is not trained yet.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.1361)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no need to any additional operation to get the loss because the expected result should be one for the correct index zero for the others.\n",
    "loss = -probs[torch.arange(len(Y)), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Write the more respectable version`:**\n",
    "**Before move on,do it by just checking the video.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 =torch.randn(6,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 6])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X].view(-1,6)\n",
    "W1 =torch.randn(6,100)\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn(100,32)\n",
    "b = torch.randn(32)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.9217e-01, -6.6178e-01,  6.1124e-01, -6.0231e-01, -1.0537e+00,\n",
       "         -6.6607e-01, -2.4958e-01, -1.3112e-01, -1.0556e+00, -1.0644e+00,\n",
       "         -4.0786e-01,  1.8293e-02, -6.6983e-02, -6.4250e-01, -4.3001e-01,\n",
       "         -6.1038e-02,  1.8944e-01, -7.5578e-01, -8.5329e-01,  6.0211e-01,\n",
       "          7.1527e-01, -1.6008e+00, -5.7841e-01, -3.9869e-01, -2.2066e-01,\n",
       "         -1.0323e+00, -8.2252e-01,  1.1279e+00,  1.8025e+00, -1.4047e+00,\n",
       "          6.1778e-02,  6.4445e-01,  1.0657e+00, -1.8962e-02, -1.2244e+00,\n",
       "          2.8159e-01, -2.3187e-02,  1.4662e+00, -4.9876e-01, -1.1359e+00,\n",
       "         -6.7188e-01,  2.4340e-01, -6.6283e-01, -8.6735e-01,  2.6425e+00,\n",
       "          3.4549e-01,  1.3021e-01, -1.7434e+00,  1.1922e+00,  1.1067e+00,\n",
       "         -2.3990e-01, -2.8501e-01, -5.5344e-01,  1.2156e+00, -1.8077e-01,\n",
       "          1.3447e+00, -2.2380e+00, -2.1846e+00, -1.2733e+00,  4.2082e-01,\n",
       "         -4.6160e-01,  9.3822e-01, -3.7169e-01,  1.4554e+00, -3.0614e+00,\n",
       "         -1.4998e+00, -7.8042e-01,  2.5862e-01, -3.6723e-01,  5.8782e-01,\n",
       "          1.2982e+00,  1.4479e+00, -8.2448e-01,  8.7249e-01, -2.2270e+00,\n",
       "         -5.4441e-01, -5.4199e-03, -9.2902e-01,  2.0268e-02,  1.1250e+00,\n",
       "          1.1502e-01, -8.8647e-02,  1.1380e+00,  1.7469e+00, -2.1136e-02,\n",
       "         -8.8463e-01,  7.9345e-01, -1.3379e+00, -2.2377e+00, -2.5930e-01,\n",
       "          4.0663e-01, -2.0517e-01, -8.2418e-01, -1.3409e-01,  2.3630e+00,\n",
       "         -3.2733e-01, -2.2734e-01, -7.0411e-01, -1.2587e-01,  1.7378e+00],\n",
       "        [ 1.1685e-01, -1.1145e+00, -1.2896e+00, -1.3422e+00, -1.2638e+00,\n",
       "          1.3898e+00, -1.0444e+00,  4.5065e-01, -2.1865e+00,  2.6595e+00,\n",
       "          6.7362e-01,  6.8926e-01,  2.0079e-02,  5.2330e-01,  1.4917e+00,\n",
       "          3.4326e-01,  4.0023e-01,  1.2616e+00, -9.7678e-01, -2.7472e-01,\n",
       "          8.2240e-01,  5.6927e-01,  6.4050e-01,  6.1515e-01, -4.0737e-01,\n",
       "          5.4685e-01, -4.0053e-01,  1.0188e+00, -1.3141e+00, -3.5161e-01,\n",
       "          8.3112e-01,  1.0165e+00,  1.1462e+00, -3.9504e-01, -6.6195e-01,\n",
       "         -4.2118e-01, -3.1630e-01, -1.4506e-01, -9.4488e-01, -1.7330e+00,\n",
       "          6.3184e-01,  6.3754e-01,  1.5584e+00, -1.3426e+00,  1.9702e-01,\n",
       "          2.9768e-01,  5.9893e-01, -1.1897e+00,  1.0492e-01, -1.6281e-01,\n",
       "          6.8465e-01, -9.4816e-01, -4.6428e-01, -1.3217e-01, -1.3620e+00,\n",
       "          5.4529e-01,  6.0370e-01,  6.1618e-01, -2.0867e-01, -8.4196e-01,\n",
       "         -4.3619e-02,  1.3744e-01,  8.2505e-01, -2.0358e-01, -7.8529e-01,\n",
       "         -1.0185e+00, -3.8561e-01, -2.2145e+00, -1.1028e+00,  5.9580e-01,\n",
       "         -1.3306e+00,  1.6101e-01, -4.4328e-02,  5.6076e-01,  4.4704e-01,\n",
       "          5.3967e-02,  1.8035e+00,  5.0660e-01, -2.3283e+00,  1.0754e+00,\n",
       "         -2.4693e-01, -2.8541e+00, -1.1325e+00,  1.1645e+00, -4.0026e-01,\n",
       "         -1.1813e+00,  3.0279e-01, -8.8001e-01,  7.4353e-01, -1.3557e+00,\n",
       "          9.6398e-01,  5.3672e-01,  1.2535e+00,  5.8500e-01, -8.2024e-01,\n",
       "         -2.2059e-01, -1.3190e+00, -7.7797e-01,  1.8733e+00,  8.7876e-01],\n",
       "        [ 6.4793e-01,  1.2685e+00,  1.1271e+00,  6.6605e-01, -4.4332e-01,\n",
       "          8.6345e-02, -8.8179e-01,  1.2796e+00, -5.2652e-01,  7.7475e-01,\n",
       "          2.2943e-01, -1.1244e+00, -5.2170e-01,  6.0902e-01,  1.9350e+00,\n",
       "          8.2465e-01,  1.3704e-01, -6.2766e-03,  1.5177e+00, -1.4771e+00,\n",
       "         -1.5352e-01, -1.5754e+00,  3.5124e-01,  6.8322e-01, -8.1534e-01,\n",
       "          6.7407e-01, -6.9872e-01, -7.3255e-01,  7.6216e-01, -2.7923e-01,\n",
       "         -1.0135e+00,  3.0183e-01,  1.9560e+00,  1.3471e+00, -5.0904e-02,\n",
       "          1.6298e+00,  7.3482e-01,  6.3401e-01, -1.7914e+00,  2.4886e-01,\n",
       "          2.7095e-01,  2.8805e-01,  1.0786e+00, -1.0764e+00,  2.5725e-01,\n",
       "          4.8020e-01, -7.1383e-02, -1.4537e+00, -3.7653e-01, -1.6896e+00,\n",
       "          3.0509e-01, -7.3161e-01,  7.2048e-01,  6.6032e-01,  1.7174e+00,\n",
       "          3.4997e-01,  1.1578e+00,  6.8461e-01, -1.0903e+00, -6.0472e-01,\n",
       "         -8.4909e-02, -4.8728e-01, -1.8548e-01,  9.7760e-01,  1.4593e+00,\n",
       "          6.0612e-01,  3.4306e-01, -1.4465e-01, -6.7229e-01, -1.3535e+00,\n",
       "          4.3717e-01, -1.2987e+00,  2.1147e+00,  2.1034e+00, -1.2932e+00,\n",
       "         -1.2266e+00, -2.0779e-01, -2.2309e-01, -7.6176e-01,  1.1213e+00,\n",
       "         -1.0540e-01, -5.7119e-01,  1.0774e+00,  2.1858e-01,  5.1607e-01,\n",
       "          5.3346e-01,  1.2224e+00,  2.2228e-01,  8.9725e-01, -5.2083e-01,\n",
       "          8.1456e-01,  1.4988e-01,  1.3527e-02, -3.6017e-01,  2.5010e-01,\n",
       "          5.3569e-01,  3.7531e-01,  4.3057e-03, -2.6445e-01,  1.9928e-01],\n",
       "        [-1.1091e+00,  2.2168e+00, -5.0951e-01, -8.2423e-01, -1.3095e+00,\n",
       "         -9.8060e-01, -1.1935e+00, -2.4913e-01, -4.7472e-02, -2.4785e-01,\n",
       "         -1.5585e-01, -1.4430e+00, -3.8542e-01,  3.4570e-01,  1.5284e+00,\n",
       "         -9.5044e-01, -5.1053e-02,  1.0997e+00,  2.1032e+00, -7.5951e-02,\n",
       "         -8.7787e-01, -1.2904e-01,  1.4641e+00,  9.4418e-01,  1.2157e-01,\n",
       "         -1.2429e+00,  4.5336e-01, -4.5619e-01,  9.6175e-01, -3.0207e-01,\n",
       "         -2.7753e-01, -1.8157e+00,  7.6821e-01,  5.2402e-01, -2.8592e-01,\n",
       "          9.3789e-01,  1.4126e+00, -5.1503e-01,  4.6438e-01, -6.8092e-01,\n",
       "         -7.2558e-01, -2.4491e-01,  7.1527e-01,  2.2695e-01,  4.4913e-01,\n",
       "         -6.1534e-01, -1.7521e+00,  2.7690e-01,  1.5963e+00,  7.0568e-01,\n",
       "         -3.6794e-02,  1.1660e+00,  1.9542e-01,  1.0346e+00,  9.4601e-01,\n",
       "          1.1016e+00,  5.4459e-01,  8.2555e-02,  1.1932e+00,  8.4536e-01,\n",
       "         -1.4061e-01, -1.8567e-03, -1.2760e+00, -5.9727e-01,  5.3220e-01,\n",
       "         -1.3799e+00, -1.2109e+00, -1.3162e+00,  5.1177e-01, -1.0089e+00,\n",
       "         -1.0404e+00, -4.6668e-01, -3.1878e-01, -5.9629e-01,  1.3927e+00,\n",
       "         -6.9052e-01, -7.2730e-01,  2.1976e-01,  7.1015e-01, -1.6885e+00,\n",
       "         -1.4178e+00,  8.8850e-02,  6.0013e-02, -9.8278e-02,  1.3547e+00,\n",
       "         -1.5357e+00, -1.0692e+00,  8.3460e-01, -5.9011e-03,  7.4716e-03,\n",
       "         -8.6465e-02, -8.8497e-01, -5.6105e-01, -3.6504e-01, -1.1032e+00,\n",
       "         -7.0838e-01, -2.1911e+00,  3.0456e-01,  1.6466e+00,  7.7341e-01],\n",
       "        [ 9.0554e-01, -6.3678e-02, -1.6206e+00, -1.7792e+00, -2.9043e-01,\n",
       "          3.5868e-01, -8.5906e-01,  2.4693e-01, -3.7299e-01,  1.6571e+00,\n",
       "         -4.5421e-01, -1.1956e+00, -1.7856e+00, -4.4970e-02,  5.6582e-02,\n",
       "         -8.2431e-01,  4.9910e-01, -2.3170e-01, -8.2764e-01, -3.8131e-02,\n",
       "         -2.2458e+00,  2.3414e+00,  6.7719e-01, -4.1606e-01, -7.4667e-01,\n",
       "         -5.6178e-01,  5.3151e-01,  1.7398e-01,  1.0790e+00, -4.5703e-01,\n",
       "         -8.3925e-01,  4.8409e-01,  1.4085e+00, -1.2593e-01,  7.8813e-01,\n",
       "          1.3637e-01, -9.2371e-01, -4.9008e-01, -1.8166e-01, -8.3612e-02,\n",
       "         -5.6431e-01,  1.2017e+00,  5.2909e-01, -8.8474e-01,  1.0242e-01,\n",
       "          1.8188e+00, -7.7680e-01,  1.8740e+00, -1.4206e+00, -3.2446e-01,\n",
       "          4.3472e-01, -1.0564e-01, -3.2824e-01,  6.6551e-01, -3.6223e-01,\n",
       "          1.2312e+00, -1.4333e+00, -9.5803e-01, -1.4081e-01,  1.2684e+00,\n",
       "         -1.8113e+00,  1.3114e+00,  1.7402e+00,  1.0673e+00, -3.8490e-01,\n",
       "          8.3638e-01,  4.9076e-01, -8.6080e-01, -3.0463e-01,  1.2601e+00,\n",
       "          4.9730e-01,  7.2507e-01, -1.5102e+00, -1.3566e+00, -4.1118e-01,\n",
       "          9.8624e-01,  4.9085e-01, -8.2882e-01,  1.8859e-01,  1.3900e+00,\n",
       "          1.2191e-01, -7.8932e-01,  5.1575e-01,  2.0401e+00, -4.0495e-01,\n",
       "          1.5679e+00, -9.9455e-01,  1.0339e+00, -1.0442e-01,  1.8609e+00,\n",
       "          1.0195e+00,  2.0408e+00, -1.6852e+00,  1.8017e+00,  3.6268e-01,\n",
       "          3.8514e-01,  6.7177e-01, -1.2979e-01,  1.2147e+00,  4.6047e-01],\n",
       "        [-8.8526e-01,  8.0929e-02, -5.8712e-01, -4.9646e-01, -5.3114e-02,\n",
       "          1.4265e+00,  1.2377e-01, -1.6172e+00,  1.1015e+00,  1.8182e+00,\n",
       "         -3.6474e-01,  1.6614e+00, -3.1162e-01,  9.3099e-01,  2.2789e-01,\n",
       "         -8.8075e-01, -1.1630e+00, -7.0332e-02, -1.2492e+00,  8.7521e-01,\n",
       "          6.2529e-01,  1.3286e+00, -7.5293e-01, -7.0648e-01,  1.0039e+00,\n",
       "         -1.2023e-01, -1.1265e+00,  1.9071e+00,  1.5211e+00, -6.3357e-01,\n",
       "         -2.8741e-01,  2.1028e+00,  9.7840e-03,  6.8911e-01,  1.9629e+00,\n",
       "         -5.2669e-01,  1.0467e+00,  3.3142e-01, -1.4254e-01,  1.6277e+00,\n",
       "          9.9696e-01,  1.1544e+00, -1.7943e-01, -4.2314e-01, -1.1437e+00,\n",
       "         -1.0871e+00, -5.6375e-01, -5.6332e-01, -1.4333e+00, -3.9326e-01,\n",
       "         -4.4959e-01,  5.0778e-01,  8.3382e-02,  5.3562e-03, -9.7213e-02,\n",
       "          1.6500e+00,  1.2613e+00,  8.9475e-01, -7.8052e-01,  6.5866e-01,\n",
       "         -7.7229e-01,  4.2636e-02,  6.4557e-01, -8.5944e-02, -1.8595e-01,\n",
       "          2.2186e+00, -7.1673e-01,  2.1231e+00,  6.4451e-01,  2.2863e-01,\n",
       "          1.4614e+00, -3.3499e-01, -1.1890e+00, -1.0240e+00, -7.0365e-01,\n",
       "          6.1081e-01,  1.7515e-01,  1.9801e+00,  9.9606e-02, -1.2540e-01,\n",
       "         -9.0406e-01, -1.0963e+00, -7.3443e-02,  6.3396e-01,  5.7659e-01,\n",
       "          5.3334e-01,  3.9552e-01,  7.3806e-01, -6.2825e-02, -2.8013e-01,\n",
       "         -1.0594e+00, -1.5872e+00, -6.1998e-01, -2.0188e-01, -7.2091e-01,\n",
       "         -1.0166e+00,  3.6566e-01,  9.7431e-01, -9.4262e-02,  1.1411e-01]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6804,  0.0523,  1.6804,  0.0523,  1.6804,  0.0523],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523, -0.3516, -1.1509],\n",
       "        [ 1.6804,  0.0523, -0.3516, -1.1509, -0.8601, -0.9394],\n",
       "        [-0.3516, -1.1509, -0.8601, -0.9394, -0.3316,  1.2129],\n",
       "        [-0.8601, -0.9394, -0.3316,  1.2129, -0.3516, -1.1509],\n",
       "        [-0.3316,  1.2129, -0.3516, -1.1509, -0.0508, -0.9616],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523,  1.6804,  0.0523],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523, -0.3516, -1.1509],\n",
       "        [ 1.6804,  0.0523, -0.3516, -1.1509, -0.8601, -0.9394],\n",
       "        [-0.3516, -1.1509, -0.8601, -0.9394, -0.3316,  1.2129],\n",
       "        [-0.8601, -0.9394, -0.3316,  1.2129, -0.5565, -1.9247],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523,  1.6804,  0.0523],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523, -0.3516, -1.1509],\n",
       "        [ 1.6804,  0.0523, -0.3516, -1.1509, -0.8601, -0.9394],\n",
       "        [-0.3516, -1.1509, -0.8601, -0.9394, -0.3316,  1.2129],\n",
       "        [-0.8601, -0.9394, -0.3316,  1.2129, -0.0602,  0.1675],\n",
       "        [-0.3316,  1.2129, -0.0602,  0.1675, -0.0508, -0.9616],\n",
       "        [-0.0602,  0.1675, -0.0508, -0.9616, -0.0508, -0.9616],\n",
       "        [-0.0508, -0.9616, -0.0508, -0.9616, -0.3516, -1.1509],\n",
       "        [-0.0508, -0.9616, -0.3516, -1.1509, -1.8883,  0.9288],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523,  1.6804,  0.0523],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523, -0.3516, -1.1509],\n",
       "        [ 1.6804,  0.0523, -0.3516, -1.1509, -0.8601, -0.9394],\n",
       "        [-0.3516, -1.1509, -0.8601, -0.9394, -0.3316,  1.2129],\n",
       "        [-0.8601, -0.9394, -0.3316,  1.2129, -0.0602,  0.1675],\n",
       "        [-0.3316,  1.2129, -0.0602,  0.1675, -1.1131, -0.6686],\n",
       "        [-0.0602,  0.1675, -1.1131, -0.6686, -1.1131, -0.6686],\n",
       "        [-1.1131, -0.6686, -1.1131, -0.6686, -0.3516, -1.1509],\n",
       "        [-1.1131, -0.6686, -0.3516, -1.1509, -1.8883,  0.9288],\n",
       "        [-0.3516, -1.1509, -1.8883,  0.9288, -0.8761,  0.0917],\n",
       "        [-1.8883,  0.9288, -0.8761,  0.0917, -0.3516, -1.1509],\n",
       "        [-0.8761,  0.0917, -0.3516, -1.1509,  0.5387, -0.1058],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523,  1.6804,  0.0523],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523, -0.3516, -1.1509],\n",
       "        [ 1.6804,  0.0523, -0.3516, -1.1509, -0.8601, -0.9394],\n",
       "        [-0.3516, -1.1509, -0.8601, -0.9394, -0.3316,  1.2129],\n",
       "        [-0.8601, -0.9394, -0.3316,  1.2129,  0.9087, -0.5112],\n",
       "        [-0.3316,  1.2129,  0.9087, -0.5112, -0.0508, -0.9616],\n",
       "        [ 0.9087, -0.5112, -0.0508, -0.9616, -0.3516, -1.1509],\n",
       "        [-0.0508, -0.9616, -0.3516, -1.1509, -0.0508, -0.9616],\n",
       "        [-0.3516, -1.1509, -0.0508, -0.9616, -0.5565, -1.9247],\n",
       "        [-0.0508, -0.9616, -0.5565, -1.9247, -0.8761,  0.0917],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523,  1.6804,  0.0523],\n",
       "        [ 1.6804,  0.0523,  1.6804,  0.0523, -0.3516, -1.1509],\n",
       "        [ 1.6804,  0.0523, -0.3516, -1.1509, -0.8601, -0.9394],\n",
       "        [-0.3516, -1.1509, -0.8601, -0.9394, -0.3316,  1.2129],\n",
       "        [-0.8601, -0.9394, -0.3316,  1.2129,  0.9087, -0.5112],\n",
       "        [-0.3316,  1.2129,  0.9087, -0.5112, -0.0508, -0.9616],\n",
       "        [ 0.9087, -0.5112, -0.0508, -0.9616, -0.3516, -1.1509],\n",
       "        [-0.0508, -0.9616, -0.3516, -1.1509,  1.9026,  1.4019],\n",
       "        [-0.3516, -1.1509,  1.9026,  1.4019, -0.5565, -1.9247],\n",
       "        [ 1.9026,  1.4019, -0.5565, -1.9247, -0.8761,  0.0917]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0154e-33, 4.7881e-16, 1.6726e-27, 1.3335e-02, 9.5756e-19, 6.3358e-35,\n",
       "        4.0154e-33, 4.7881e-16, 1.6726e-27, 4.3340e-38, 5.0287e-34, 4.0154e-33,\n",
       "        4.7881e-16, 1.6726e-27, 1.5675e-34, 2.1281e-30, 3.4889e-20, 7.3150e-30,\n",
       "        2.4121e-31, 1.5839e-20, 4.0154e-33, 4.7881e-16, 1.6726e-27, 1.5675e-34,\n",
       "        8.3237e-32, 8.1730e-28, 2.1254e-30, 2.0603e-35, 5.1965e-28, 6.7585e-32,\n",
       "        1.5540e-22, 1.2176e-29, 4.0154e-33, 4.7881e-16, 1.6726e-27, 1.0678e-18,\n",
       "        5.3318e-20, 6.5975e-40, 1.7326e-10, 7.3788e-33, 4.1279e-08, 1.6297e-23,\n",
       "        4.0154e-33, 4.7881e-16, 1.6726e-27, 1.0678e-18, 5.3318e-20, 6.5975e-40,\n",
       "        1.1998e-18, 1.5697e-41, 9.8666e-01, 1.8612e-30])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.4950, -0.5382, -0.1393,  ..., -1.0708,  1.8940,  4.4151],\n",
      "        [ 2.7201, -0.5062,  3.8601,  ..., -1.9793, -0.4609,  3.3421],\n",
      "        [ 2.0902, -5.7015,  2.8828,  ..., -2.0824, -2.5423,  1.7966],\n",
      "        ...,\n",
      "        [ 0.8564, -3.4123, -2.7196,  ...,  1.8478, -1.0885, -0.5635],\n",
      "        [-0.0869,  5.4036,  4.5189,  ...,  0.0722, -0.4716, -0.3411],\n",
      "        [ 2.2442, -9.2434,  0.8620,  ..., -2.5186, -1.3786,  2.8398]])\n",
      "logits shape: torch.Size([52, 32])\n",
      "counts:tensor([[1.9550e-06, 1.7232e-04, 6.9490e+21,  ..., 1.0960e-09, 1.3534e+19,\n",
      "         1.7559e-01],\n",
      "        [7.2328e-08, 1.2383e-20, 2.0548e+13,  ..., 2.0266e-07, 4.0671e+00,\n",
      "         5.4682e-05],\n",
      "        [1.2226e-02, 2.5902e-03, 1.3144e+17,  ..., 3.4877e-14, 5.6641e+02,\n",
      "         1.9602e+07],\n",
      "        ...,\n",
      "        [4.1072e+05, 3.0748e+33, 1.2496e+05,  ..., 7.2053e-15, 3.3731e+16,\n",
      "         3.4145e+18],\n",
      "        [4.7686e-08, 3.1261e-34, 5.9116e-13,  ..., 4.2657e+06, 7.5231e-23,\n",
      "         3.4377e-03],\n",
      "        [7.9875e-02, 1.0490e+10, 9.6632e+23,  ..., 4.1949e-20, 1.4553e+05,\n",
      "         1.5495e+05]])\n",
      "probs:tensor([[2.6949e-39, 2.3754e-37, 9.5789e-12,  ..., 1.5106e-42, 1.8657e-14,\n",
      "         2.4205e-34],\n",
      "        [2.1019e-44, 0.0000e+00, 5.7846e-24,  ..., 5.7453e-44, 1.1449e-36,\n",
      "         1.5393e-41],\n",
      "        [8.2155e-23, 1.7406e-23, 8.8323e-04,  ..., 2.3436e-34, 3.8061e-18,\n",
      "         1.3172e-13],\n",
      "        ...,\n",
      "        [1.3358e-28, 1.0000e+00, 4.0639e-29,  ..., 0.0000e+00, 1.0970e-17,\n",
      "         1.1105e-15],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.5771e-30, 2.0713e-19, 1.9080e-05,  ..., 0.0000e+00, 2.8735e-24,\n",
      "         3.0595e-24]])\n",
      "torch.Size([52, 32])\n",
      "loss: -0.024864425882697105\n"
     ]
    }
   ],
   "source": [
    "h = emb @ W1 + b1\n",
    "print(h)\n",
    "logits = h @ W2 + b2\n",
    "print(f'logits shape: {logits.shape}')\n",
    "counts = logits.exp()\n",
    "print(f'counts:{counts}')\n",
    "probs = counts / counts.sum(-1, keepdim=True)\n",
    "print(f'probs:{probs}'),print(probs.shape)\n",
    "\n",
    "loss = -probs[torch.arange(52),Y].mean()\n",
    "print(f'loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
